% Time-stamp: <2022-11-03 15:06:34 ubuntu>
% Romain Lafarguette 2020, https://romainlafarguette.github.io/

%% ---------------------------------------------------------------------------
%% Preamble: Packages and Setup
%% ---------------------------------------------------------------------------
% Class 
\documentclass{beamer}

% Font and encoding
\usepackage[utf8]{inputenc} % Input font
\usepackage[T1]{fontenc} % Output font
\usepackage{lmodern} % Standard LateX font
\usefonttheme{serif} % Standard LateX font

% Maths 
\usepackage{amsfonts, amsmath, mathabx, bm, bbm} % Maths Fonts

% Graphics
\usepackage{graphicx} % Insert graphics
\usepackage{subfig} % Multiple figures in one graphic
\graphicspath{{/../static/img}{/../static/diagrams}}

% Layout
\usepackage{changepage}

% Colors
\usepackage{xcolor}
\definecolor{imfblue}{RGB}{0,76,151} % Official IMF color
\setbeamercolor{title}{fg=imfblue}
\setbeamercolor{frametitle}{fg=imfblue}
\setbeamercolor{structure}{fg=imfblue}
\setbeamercolor{page number in head/foot}{fg=imfblue}
\setbeamerfont{page number in head/foot}{size=\footnotesize}

% Tables
\usepackage{booktabs,rotating,multirow} % Tabular rules and other macros
\usepackage{pdflscape,afterpage} % Landscape mode and afterpage
\usepackage{threeparttable} % Split long tables
\usepackage[font=scriptsize,labelfont=scriptsize,labelfont={color=imfblue}]{caption}

% Import files
\usepackage{import}

% Appendix slides
\usepackage{appendixnumberbeamer} % Manage page numbers for appendix slides

% References
\usepackage{hyperref}

% A few macros: environments
\newenvironment{wideitemize}{\itemize\addtolength{\itemsep}{10pt}}{\enditemize}
\newenvironment{wideenumerate}{\enumerate\addtolength{\itemsep}{10pt}}{\endenumerate}

\newenvironment{extrawideitemize}{\itemize\addtolength{\itemsep}{30pt}}{\enditemize}
\newenvironment{extrawideenumerate}{\enumerate\addtolength{\itemsep}{30pt}}{\endenumerate}

% Define the footer with higher/lower adjustment
\defbeamertemplate{footline}{higher page number}
{
  \hfill
  \usebeamercolor[fg]{page number in head/foot}
  \usebeamerfont{page number in head/foot}  
  \thepage/\inserttotalframenumber\kern1em\vskip2pt %Change xxpt to
                                %lower/higher the footnote
}
\setbeamertemplate{footline}[higher page number]

% Remove navigation symbols and other superfluous elements
\setbeamertemplate{navigation symbols}{}
\setbeameroption{hide notes}
\setbeamertemplate{note page}[plain]
\beamertemplatenavigationsymbolsempty
\hypersetup{pdfpagemode=UseNone} % don't show bookmarks on initial view

% Institute font
\setbeamerfont{institute}{size=\footnotesize}
\DeclareMathSizes{10}{9}{7}{5}  

%% ---------------------------------------------------------------------------
%% Title info
%% ---------------------------------------------------------------------------
\title[Concepts]{Introduction to Time Series Econometrics}
\author[R. Lafarguette]{Romain Lafarguette, Ph.D.}
\institute[IMF]{Quant \& IMF External Consultant}

\date[STI, 08 Nov 2022]{Singapore Training Institute, 08 November 2022}

\titlegraphic{
    \begin{figure}
    \centering
    \subfloat{{\includegraphics[width=2cm]{../static/img/imf_logo}}}%
    \end{figure}
}

%% ---------------------------------------------------------------------------
%% Title slide
%% ---------------------------------------------------------------------------
\begin{document}

\begingroup
\renewcommand{\insertframenumber}{}
\begin{frame}
  %\addtocounter{framenumber}{-1}
\maketitle
\end{frame}
\endgroup


\begin{frame}
  \frametitle{Outline}
  \begin{enumerate}
  \item \textbf{Data concepts}: population, sample, data types, data generating process, etc.
  \item \textbf{Estimation strategy}
  \end{enumerate}


\smallskip
\emph{NB: this slide-deck is inspired by the excellent website of \url{https://sites.google.com/view/christophe-hurlin/teaching-resources}{Christophe Hurlin}}  
\end{frame}



%% ---------------------------------------------------------------------------
%% Core
%% ---------------------------------------------------------------------------

\begin{frame}
  \frametitle{Overview}

Financial econometrics (including time-series econometrics) are based on four main elements:\\
\smallskip

  \begin{wideenumerate}
    \item A sample of data
    \item An econometric model, based on a theory or not
    \item An estimation method to estimate the coefficients of the model
    \item Inference/testing approach to validate the estimation
  \end{wideenumerate}
  
\end{frame}



\section{Data Concepts}
\begin{frame}
  \frametitle{Population vs. Sample}

  \begin{block}{Definition: Population}
    A \textbf{population} is defined as including all entities (e.g. banks or firms) or all the time periods of the processus that has to be explained
  \end{block}

\smallskip
  
  \begin{wideitemize}
    \item In most cases, it is impossible to observe the entire statistical population, due to constraints (recording period, cost, etc.)
    \item A researcher would instead observe a \textbf{statistical sample} from the population. He will estimate an econometric model to understand the \textbf{properties on the population as a whole}.
  \end{wideitemize} 
\end{frame}


\begin{frame}
  \frametitle{Data Generating Process}
  
  \begin{block}{Definition: Data Generating Process}
    A \textbf{Data Generating Process (DGP)} is a process in the real world that "generates" the data (or the sample) of interest
  \end{block}

  \begin{exampleblock}{Example: Data Generating Process}
    Let us assume that there is a linear relationship between interest rates in two countries ($R, R^*$), their forward ($F$) and their spot exchange rate ($S$).\\

    \begin{equation*}
      \frac{F}{S} = \frac{1+R}{1+R^*}
    \end{equation*}

    This non-arbitrage relationship (CIP) can be used in the foreign exchange market to determine the forward exchange rate

    \begin{equation*}
      \mathbb{E}[F |S=s, R=r, R^*= r^*] = s*\frac{1+r}{1+r^*}
    \end{equation*}

This relationship is the \textbf{Data Generating Process} for $F$     
  \end{exampleblock}

The equivalent of population for time series econometrics is the DGP.\\
NB: note that I use $R$ to describe the random variable and $r$ to describe its realization
  
\end{frame}


\begin{frame}
  \frametitle{Econometrics Challenge}
  The challenge of econometrics is to draw conclusions about a DGP (or population), after observing only one realization $\{x_1, \dots X_N\}$ of a random sample (the dataset).\\

%TODO: add a diagram
  
\end{frame}


\begin{frame}
  \frametitle{Data Types}

  In econometrics, sets can be mainly distinguished in three types:\\
  \smallskip
  
  \begin{wideenumerate}
    \item Cross-sectional data
    \item Time series data
    \item Panel data
  \end{wideenumerate}
  
\end{frame}

% ADD some diagrams
\begin{frame}
  \frametitle{Cross-Sectional Data}
  Cross-sectional data are the most common type of data encountered in statistics and econometrics.\\ 
  \smallskip
  
  \begin{wideitemize}
    \item Data at the entities level: banks, countries, individuals, households, etc.
    \item \textbf{No time dimension}: only one "wave" or multiple waves of different entities
    \item Order of data does not matter: no time structure
  \end{wideitemize}
\end{frame}


\begin{frame}
  \frametitle{Time Series Data}

  Time series data are very common in financial econometrics and central banking. They entail specific estimation methods to do the \textbf{time-dependence}.\\
  
  \begin{wideitemize}
    \item Data for a single entity (person, bank, country, etc.) collected at multiple time periods. Repeated observations of the same variables (interest rate, GDP, prices, etc.)
    \item Order of data is important!
    \item The observations are typically not independent over time
    \item In this case, the notion of population corresponds to the \textbf{Data Generating Process (DGP)} 
  \end{wideitemize}  
\end{frame}


\begin{frame}
  \frametitle{Panel Data}
  Also called longitudinal data. They contain the most information and allow for more complex estimation and analysis.

  \begin{wideitemize}
    \item Data for multiple entities (individuals, firms, countries, banks, etc.) in which outcomes and characteristics of each entity are observed at multiple points in time
    \item Combine cross-sectional and time-series information
    \item Present several advantages with respect to cross-sectional and time series data, depending on the topic at hands
  \end{wideitemize}

\end{frame}


\begin{frame}
  \frametitle{Econometric Model}

  \begin{block}{Definition: Econometric Model}
    An econometric model specificies the statistical relationship between different economic variables, that are expected to be stable over time
  \end{block}

  \begin{enumerate}
  \item \textbf{Parametric model:} fully characterization of the relationship by a \textbf{set of parameters} $\theta$ and a \textbf{link function} $f$ supposed to be known; the specification can be linear or non linear, and includes some randomness $\epsilon$
    \begin{equation*}
      Y = f(X; \theta) + \epsilon
    \end{equation*}

  \item \textbf{Non-parametric and semi-parametric models}: the link function can not be described using a finite number of parameters. The link function is assumed to be unknown and has to be estimated
    
  \end{enumerate}
  
\end{frame}


\section{Financial Econometrics}

\begin{frame}
  \frametitle{Empirical Strategy}
  The general approach of (financial) econometrics is as follows:\\
  \smallskip

  \begin{wideenumerate}
  \item Specification of the model
  \item Estimation of the parameters
  \item Diagnostic tests
    \begin{itemize}
    \item Significance tests
    \item Specification tests
    \item Backtesting tests
    \item etc.
    \end{itemize}
  \item Interpretation and use of the model (forecasting, historical studies, etc.)
  \end{wideenumerate}
  
\end{frame}


\begin{frame}
  \frametitle{Refresher: Random Variables}
  \begin{itemize}
  \item Mathematicians are formalizing and modeling randomness via the concept of \textbf{random variables}.\\
  \item Pay attention: a random variable is neither random (it is formalized via laws and distributions), not a variable (it is a function)
  \item A \textbf{random variable} is a function $f: \Omega \mapsto \mathcal{R}$ that assigns to
      a set of outcome $\Omega$ a \textbf{value}, often a real number.
  \item The probability of an outcome is equal to its \textbf{measure} divided by
        the measure of all possible outcomes
     \begin{itemize}
      \item Example: obtaining an even number by rolling a dice: $\{2, 4, 6\}$
      \item Probability to obtain an even number by rolling a dice: $m(\{2, 4,
        6\})/m(\{1, 2, 3, 4, 5, 6\}) = \frac{1}{2}$ \emph{(here, the measure
          simply "counts" the outcomes with equal weights)}
        \end{itemize}     
  \end{itemize}  
\end{frame}

\begin{frame}
  \frametitle{Random Variables (II)}    
  \begin{itemize}
  \item Random variables are the "building block" of statistics:
    \begin{itemize}
    \item Random variables are characterized by their distribution (generating
      function, moments, quantiles, etc.)
    \item The behavior of two or more random variables can be characterized by
      their dependence/independence, matrix of variance-covariance, joint
      distribution, etc.
    \item The main theorems in statistics (law of large numbers, central limit
      theorem, etc.) leverage the properties of random variables
    \end{itemize}
  \end{itemize}
\end{frame}


\begin{frame}
  \frametitle{Refresher: Probability Distributions for Continuous Variables}
  
  \begin{block}{Definition: Probability Distribution}
    The probability distribution of a random variable describes how the probabilities of the outcomes are distributed. How more likely is one outcome compared to another? Is there more upside risk or downside risk? Etc.
  \end{block}

  Distributions are equivalently represented by:

  \begin{itemize}
  \item \textbf{The probability density function (pdf)} usually denoted $f_X(x0)$ (remember than $X$ stands for the random variable, while $x0$ stands for a realization of the random variable) represents the relatively likelihood that the random variable $X$ will fall within a small neighborhood of $x0$ (infinitesimal concept). It is easier to conceptualize the pdf via the cdf
  \item \textbf{The cumulative density function (cdf)} usually denoted $F_X(x0)$ represents the probability that the random variable will be lower than $x0$. It cumulates the pdf ("all the small neighborhoods") such that:
    \begin{equation*}
F_X(x0) \equiv \mathbb{P}[X \leq x0] = \int_{-infty}^{x0} f_X(h) dh      
    \end{equation*}    
  \end{itemize}
  
\end{frame}


\begin{frame}
  \frametitle{Moments: Overview}
  \begin{table}
    \centering
    \begin{tabular}{lll}
      \hline
      \hline
      & Formula & Interpretation \\
      \hline
      \textbf{Mean} & $\mathbb{E}[X_t] = \mu$ & Central tendency\\
      \textbf{Variance} & $\mathbb{V}[X] = \mathbb{E}\left[(X_t - \mu)^2\right] = \sigma^2$ & Dispersion around $\mu$\\
      \textbf{Skewness} & $\mathbb{S}[X] = \mathbb{E}\left[(X_t - \mu)^3\right] = \text{sk}$ & Symmetry\\
      \textbf{Kurtosis} & $\mathbb{K}[X] = \mathbb{E}\left[(X_t - \mu)^4\right] = \kappa$ & Tail heaviness\\
      \hline
      \hline                                                                                            
    \end{tabular}
  \end{table}
\end{frame}


% TODO: add charts

\begin{frame}
  \frametitle{Moments: In Practice}

  \begin{wideitemize}
    \item The moments allow to characterize the shape of the returns distribution
    \item However, the theoretical moments are \textbf{unobservable} and need to be estimated
    \item Assume that we have a sample $\{x_1, \dots, x_T\}$ realizations of the sequence of $X_t$
  \end{wideitemize}
  
\end{frame}



\begin{frame}
  \frametitle{Estimator}

  \begin{block}{Definition: Estimator}
    An \textbf{estimator} is any function $F(x1, \dots, x_t)$ of a sample. Note that any descriptive statistics is an estimator (a simple one) 
  \end{block}


  \begin{exampleblock}{Example: Sample Mean}
    The sample mean (or overage) of a sample is an estimator of the (theoretical) mean $ \mathbb{E}[X_t] = \mu$.\\
    The estimator is simply: $\hat{\mu_t} \equiv \bar{X_t} = \frac{1}{T} \sum_{t=1}^{T}x_t$
  \end{exampleblock}
  
\end{frame}


\begin{frame}
  \frametitle{Example: Variance}

  \begin{exampleblock}{Example: Sample Mean}
    Assume that the observations are drawn from \emph{i.i.d} random variables.\\
    The \textbf{sample variance} $\hat{sigma}^2_T = \frac{1}{T-1} \sum_{t=1}^T (x_t - \bar{x}_t)^2$
  \end{exampleblock}

\textbf{Note:} the denominator is equal to \emph{T-1} as to define a sample variance corrected for the small sample bias. 
  
\end{frame}



\begin{frame}
  \frametitle{Sampling Distribution}

  \begin{block}{Fact}
    An estimator $\hat{\theta}$ is a \textbf{random variable}
  \end{block}
  Therefore, $\hat{\theta}$ has a (marginal or conditional) \textbf{probability distribution}. This sampling distribution is characterized by a probability distribution function (pdf) $f_{\hat{\theta}}(u)$

  \begin{block}{Definition: Sampling Distributions}
    The probability distribution of an estimator is called the \textbf{sampling distribution}
  \end{block}
  The sampling distribution is described by its moments, such as expectations, variance, skewness, etc.
  \end{frame}


  \begin{frame}
    \frametitle{Point Estimate}
    \begin{block}{Definition: Estimate}
      An estimate is the realized value of an estimator (e.g. a number, in a case of a point estimate) that is obtained when a sample is actually taken.\\
      For an estimator $\hat{\theta}$, it can be denoted by $\hat{\theta}(y)$
    \end{block}

    \begin{exampleblock}{Estimate of a linear regression}
      \begin{itemize}
      \item We are interested in the following DGP: $Y = \alpha + \beta*X + \epsilon$, where we observe a joint sample ${y1, \dots, y_T}, {x1, \dots, x_T}$.\\
      \item We have an estimator (for instance an OLS) of $\hat{\alpha}, \hat{\beta}$
      \item Then, for any value of $X=x_0$, we can simply project the \textbf{conditional expected estimate} $y_0 = \hat{\alpha} + \hat{\beta}*x_o + \hat{\epsilon}$
      \item If the estimator is unbiased, the fitted residuals $\hat{\epsilon} = y_t - \hat{\alpha} - \hat{\beta}*x_t$ are centered on \textbf{average}:$\mathbb{E}\left[\hat{\epsilon}\right]=0$. This is why the residual disappear from the estimate of the conditional expected estimate in an OLS... but no bias doesn't mean no variance !
      \item $\epsilon$ and consequently $\hat{\epsilon}$ are random variables: their distribution will determine the distribution of the estimator $\hat{\theta} = \hat{\alpha}, \hat{\beta}$
      \end{itemize}
    \end{exampleblock}    
  \end{frame}


  \begin{frame}
    \frametitle{What Constitutes a Good Estimator?}
    The idea is to study the properties of the \textbf{sampling distribution} of the estimator $\hat{\theta}$, and especially its moments, such as:

    \begin{wideitemize}
    \item $\mathbb{E}[\hat{\theta}]$ for the biais
    \item $\mathbb{V}[\hat{\theta}]$ for the precision
    \item $\mathbb{S}[\hat{\theta}]$ for the symmetry
    \item $\mathbb{K}[\hat{\theta}]$ for the tail-risks  
    \item etc.
    \end{wideitemize}
  \end{frame}


  \begin{frame}
    \frametitle{Estimators Properties}
    Estimators are compared on the basis of a variety of attributes

    \begin{wideenumerate}
      \item \textbf{Finite sample properties} (or finite sample distributions) investigate how the estimator behave when the observational sample is relatively limited (a few hundreds to a few thousands of observations max)
      \item However, these properties rely on a distributional assumption (usually normality or gaussianity), that may be difficult to test. 
      \item When the normality assumption is no longer valid (and the finite sample distribution is unknown), estimators are evaluated on the basis on their \textbf{large sample}, or \textbf{asymptotic properties}
    \end{wideenumerate}
    
  \end{frame}



  \begin{frame}
    \frametitle{Finite Sample Theorem}
    \begin{block}{Theorem: Finite Sample Distributions}
        If we assume that, with a sample of size $T$, generated from a stochastic process with i.i.d random variables $X_1, X_2, \dots, X_T$  with $X_{t} \sim \mathcal{N}(\mu, \sigma^2)$, then the estimators of the sample mean $\hat{\mu_T}$ and the estimator of the sample variance/population variance $(T-1) \frac{\hat{\sigma^2_T}}{\sigma^2}$ have a \textbf{finite sample distribution}

        \begin{equation*}
          \hat{\mu_T} = \mathcal{N}\left(\mu, \frac{\sigma^2}{T} \right) \qquad \forall \ T \ \in \ \mathbb{N}
        \end{equation*}


        \begin{equation*}
          \frac{T-1}{\sigma^2} \hat{\sigma^2} \sim \chi^{2} (T-1) \qquad \forall T \ \geq \ 2
        \end{equation*}
        
      \end{block}


      \begin{exampleblock}{Example: Finite Sample Distribution}
        Under the Gaussianity assumption, with a sample size of $T = 10$, then:
        \begin{equation*}
          \hat{\mu} \sim \mathcal{N}\left( \mu, \frac{\sigma^2}{10} \right) \qquad 9 \times \frac{\hat{sigma^2_T}}{\sigma^2} \sim \chi^2(9)
        \end{equation*}        
      \end{exampleblock}
      
  \end{frame}
  

  \begin{frame}
    \frametitle{Difficulties with Finite Sample Inference}
    In most cases, it is impossible to derive the \textbf{exact/finite sample distribution} for the estimator (or any transformation of the estimator).

    \begin{wideenumerate}
    \item In some cases, the exact distribution of $\{ X1, X2, \dots, X_T\}$ is known, but the estimator function $S()$ (also called the "statistics") is too complicated
      \begin{equation*}
        \hat{\theta} = S(X_1, X_2, \dots, X_T) \sim ??? \qquad \forall \ T \ \in \mathbb{N}
      \end{equation*}

      \item In most cases, the distribution of the stochastic process $\{X1, X2, \dots, X_T\}$ is unknown

      \begin{equation*}
        \hat{\theta} = S(X_1, X_2, \dots, X_T) \sim ??? \qquad \forall \ T \ \in \mathbb{N}
      \end{equation*}
        
    \end{wideenumerate}
    
  \end{frame}
  


  \begin{frame}
    \frametitle{Asymptotic Properties}
    
  \end{frame}
  
  
\end{document}
  
\section{Misc}

\begin{frame}
  \frametitle{Stochastic Process}

  \begin{itemize}
  \item A stochastic process is a sequence of random variables indexed by time
    ($t$):
    \begin{equation*}
      {\dots, Y_1, Y_2, \dots, Y_t, Y_{t+1}, \dots} = 
    \end{equation*}
    
  \end{itemize}
  

  
\end{frame}


\begin{frame}
  \frametitle{Stationarity}
  
\end{frame}


\begin{frame}
  \frametitle{Ergodicity}
  
\end{frame}


\begin{frame}
  \frametitle{Moments}
  
\end{frame}


\begin{frame}
  \frametitle{Estimator}
  
\end{frame}


\begin{frame}
  \frametitle{Convergence}
  
\end{frame}

\begin{frame}
  \frametitle{Biais}
  
\end{frame}


\begin{frame}
  \frametitle{Efficiency}
  
\end{frame}




%% ---------------------------------------------------------------------------
%% End document
%% ---------------------------------------------------------------------------
\end{document}


%% ---------------------------------------------------------------------------
%% Sample code
%% ---------------------------------------------------------------------------

% \begin{frame}
% \frametitle{Example with a Figure}
%     \makebox[\linewidth]{\includegraphics[width=\paperwidth]{../output/step_007_top_ae_dotplot.pdf}}
% \end{frame}


% \begin{frame}
% \begin{adjustwidth}{-2em}{-2em} % Wider frame 
%   \frametitle{Example with a Wide Table}  
%   \setlength\tabcolsep{4pt}  % default value: 6pt
%   %\footnotesize
%   \centering
%   \input{../output/mytable.tex}\\
% \end{adjustwidth}
% \end{frame}









%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
