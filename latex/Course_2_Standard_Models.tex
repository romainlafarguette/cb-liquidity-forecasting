% Time-stamp: <2022-11-04 14:24:26 ubuntu>
% Romain Lafarguette 2020, https://romainlafarguette.github.io/

%% ---------------------------------------------------------------------------
%% Preamble: Packages and Setup
%% ---------------------------------------------------------------------------
% Class 
\documentclass{beamer}

% Theme
\usetheme{Boadilla}
\usecolortheme{dolphin}
%\setbeamertemplate{headline}{} % Remove the top navigation bar

% Font and encoding
\usepackage[utf8]{inputenc} % Input font
\usepackage[T1]{fontenc} % Output font
\usepackage{lmodern} % Standard LateX font
\usefonttheme{serif} % Standard LateX font

% Maths 
\usepackage{amsfonts, amsmath, mathabx, bm, bbm} % Maths Fonts

% Graphics
\usepackage{graphicx} % Insert graphics
\usepackage{subfig} % Multiple figures in one graphic
\graphicspath{{/../static/img}{/../static/diagrams}{/../static/course_1_img}}

% Layout
\usepackage{changepage}

% Colors
\usepackage{xcolor}
\definecolor{imfblue}{RGB}{0,76,151} % Official IMF color
\setbeamercolor{title}{fg=imfblue}
\setbeamercolor{frametitle}{fg=imfblue}
\setbeamercolor{structure}{fg=imfblue}
\setbeamercolor{page number in head/foot}{fg=imfblue}
\setbeamerfont{page number in head/foot}{size=\footnotesize}

% Tables
\usepackage{booktabs,rotating,multirow} % Tabular rules and other macros
\usepackage{pdflscape,afterpage} % Landscape mode and afterpage
\usepackage{threeparttable} % Split long tables
\usepackage[font=scriptsize,labelfont=scriptsize,labelfont={color=imfblue}]{caption}

% Import files
\usepackage{import}

% Appendix slides
\usepackage{appendixnumberbeamer} % Manage page numbers for appendix slides

% References
\usepackage{hyperref}

% A few macros: environments
\newenvironment{wideitemize}{\itemize\addtolength{\itemsep}{10pt}}{\enditemize}
\newenvironment{wideenumerate}{\enumerate\addtolength{\itemsep}{10pt}}{\endenumerate}

\newenvironment{extrawideitemize}{\itemize\addtolength{\itemsep}{30pt}}{\enditemize}
\newenvironment{extrawideenumerate}{\enumerate\addtolength{\itemsep}{30pt}}{\endenumerate}

% Define the footer with higher/lower adjustment
% \defbeamertemplate{footline}{higher page number}
% {
%   \hfill
%   \usebeamercolor[fg]{page number in head/foot}
%   \usebeamerfont{page number in head/foot}  
%   \thepage/\inserttotalframenumber\kern1em\vskip2pt %Change xxpt to
%                                 %lower/higher the footnote
% }
% \setbeamertemplate{footline}[higher page number]

% Remove navigation symbols and other superfluous elements
\setbeamertemplate{navigation symbols}{}
\beamertemplatenavigationsymbolsempty

%\setbeamertemplate{note page}[plain]
\hypersetup{pdfpagemode=UseNone} % don't show bookmarks on initial view
\setbeameroption{hide notes}

% Institute font
\setbeamerfont{institute}{size=\footnotesize}
\DeclareMathSizes{10}{9}{7}{5}  

%% ---------------------------------------------------------------------------
%% Title info
%% ---------------------------------------------------------------------------
\title[Standard Models]{Standard Time Series Models}
\author[R. Lafarguette]{Romain Lafarguette, Ph.D.}
\institute[IMF]{ADIA Quant \& IMF External Consultant}

\date[STI, 08 Nov 2022]{Singapore Training Institute, 08 November 2022}

\titlegraphic{
    \begin{figure}
    \centering
    \subfloat{{\includegraphics[width=2cm]{../static/img/imf_logo}}}%
    \end{figure}
}

%% ---------------------------------------------------------------------------
%% Title slide
%% ---------------------------------------------------------------------------
\begin{document}

\begin{frame}
\maketitle
\end{frame}

% \begin{frame}
%   \frametitle{Outline}

%   This short crash-course is divided in three parts covering:\\
%   \smallskip
  
%   \begin{enumerate}
%   \item \textbf{Data concepts}: population, sample, data types, data generating process, etc.
%   \item \textbf{Statistical Inference}: stochastic process, estimator, distributions, convergence, etc.
%   \item \textbf{Statistical Properties of Financial Time Series}: stationarity, autocorrelation, heavy tails, etc.
%   \end{enumerate}


% \smallskip
% \emph{NB: this slide-deck follows the excellent course of Christophe Hurlin \url{https://sites.google.com/view/christophe-hurlin/teaching-resources}}  
% \end{frame}




\section{ETS Model}


% \begin{frame}{Simple Exponential Smoothing}
% Simple exponential smoothing is a weighted average of past observations, with weights decreasing exponentially

% \begin{equation*}
%   y_{t+1} = \alpha y_t + \alpha(1-\alpha) y_{t-1} + \alpha(1-\alpha)^2 y_{t-2} + \dots \qquad \text{where} \ \alpha \ \in \ [0, 1]
% \end{equation*}

% \begin{itemize}
% \item More advanced specifications allow for trends, seasonality, etc.
% \end{itemize}


% \end{frame}


\begin{frame}
  \frametitle{Perspective}
  
  \begin{itemize}
  \item ETS (Error, Trend, Season) model was developed in the 1950s as algorithms to produce point forecasts
  \item ETS combines a "level" ($l_{t-1}$), a "trend" (($b_{t-1}$)) and a "seasonal" ($s_{t-m}$) components to describe a time series
  \item The combination $f(l_{t-1}, b_{t-1}, s_{t-m})$ can be additive, multiplicative, etc.
  \item The rate of change of the components are controlled by "smoothing" parameters: $\alpha$ for the level, $\beta$ for the trend, $\gamma$ for the seasonal
  \item The researcher has to:
    \begin{enumerate}
    \item To choose the best values for the smoothing parameters 
    \item The initial state of the parameters
    \end{enumerate}
  \item Equivalent ETS state-space models have been developped in the 1990s and the 2000s
  \end{itemize}

% Under the simplest, additive specification:
%   \begin{equation*}
%     x_t = l_{t-1} + b_{t-1} + s_{t-m} + \epsilon_t
%   \end{equation*}
  
\end{frame}


\begin{frame}
  \frametitle{Combining Level, Trend and Seasonal Components}

  \begin{itemize}
  \item \textbf{Additively:} $y_t = l_{t-1} + b_{t-1} + s_{t-m} + \epsilon_t$
  \item \textbf{Multiplicatively:} $y_t = l_{t-1} \times b_{t-1} \times s_{t-m} \times (1 + \epsilon_t)$
  \item \textbf{Mixed:} $y_t = (l_{t-1} + b_{t-1}) \times s_{t-m} + \epsilon_t $  
  \end{itemize}

  Notations:
  \begin{itemize}
  \item \textbf{Error} can be additive ("A") or multiplicative ("M")
  \item \textbf{Trend} can be None ("N"), additive ("A"), multiplicative ("M") or damped ("Ad" or "Md")
  \item Seasonality can be None ("N"), additive ("A") or multiplicative ("M")
  \end{itemize}
    
\end{frame}


\begin{frame}{Point forecasts and forecast distribution}

  \begin{itemize}
  \item Models generates point forecasts estimates, based on a given conditional value $y_{t+1|y0} = f_y(y0)$ 
  \item However, need to generate the forecast distribution to assess the quality of models and select the best. \textbf{Model selection}
  \item A stochastic data generating process (DGP) can generate an entire forecast distribution
  \item Core idea: the residual ($\epsilon$) is the only stochastic (=random) element in $y_t = l_{t-1} + b_{t-1} + s_{t-m} + \epsilon_t$
    \begin{itemize}
    \item Hence, the distribution of the residuals will determine the distribution of the estimator
    \end{itemize}
  \end{itemize}


  
\end{frame}

\section{Additive Models}
\begin{frame}{Level-Only Model ETS(A, N, N) (Simple Smoothing)}
  \begin{block}{Component Form}
    \begin{itemize}
    \item Forecast equation: $\hat{y}_{t+h |t} = l_t$
    \item Smoothing equation: $l_t = \alpha y_t + (1-\alpha)l_{t-1}$
    \end{itemize}
  \end{block}

  \begin{itemize}
  \item $l_t$ is the level (="smoothed value") of the series at time $t$
  \item $\hat{y}_{t+1|t} = \alpha y_t + (1 - \alpha)\hat{y}_{t|t-1}$
  \end{itemize}
  Iterate to get exponentially weighted moving average form: $\hat{y}_{T+1|T} = \sum_{j=0}^{T-1}\alpha(1-\alpha)^j y_{T-j} + (1-\alpha)^Tl_0$  
\end{frame}

\begin{frame}{Error Correction Form}
  \begin{exampleblock}{Component Form}
    \begin{itemize}
    \item Forecast equation: $\hat{y}_{t+h |t} = l_t$
    \item Smoothing equation: $l_t = \alpha y_t + (1-\alpha)l_{t-1}$
    \end{itemize}
  \end{exampleblock}

  Forecast error: $e_t = y_t - \hat{y}_{t|t-1} = y_t - l_{t-1}$
  
  \begin{block}{Error Correction Form}
    \begin{itemize}
    \item $y_t = l_{t-1} + e_t$
    \item $l_t = l_{t-1} + \alpha*\underbrace{(y_t - l_{t-1})}_{e_t}$
    \end{itemize}

    \begin{itemize}
    \item Intuition: $\alpha$ updates the next-period estimate based on the forecasting error
    \item Specify probability distribution for $e_t$, often assumed that $e_t = \epsilon_t \sim \mathcal{N}(0, \sigma^2)$
    \end{itemize}
           
  \end{block}
  
\end{frame}


\begin{frame}
  \begin{itemize}
  \item Need to choose the best values for $\alpha$ and $l_0$
  \item Similarly to regression, choose optimal parameters by minimizing SSE:
    \begin{equation*}
      \text{SSE} = \sum_{t=1}^T (y_t - \hat{y}_{t|t-1})^2
    \end{equation*}
  \item Unlike regression, there is no closed form solution: need to use numerical optimization
  \end{itemize}
\end{frame}


\begin{frame}{State-Space Representation for Additive Models}
  \begin{block}{State-Space Representation}
    \begin{itemize}
    \item \textbf{Measurement equation}: $y_t = l_{t-1} + \epsilon_t$, where $\epsilon_t \sim \mathcal{N}(0, \sigma^2)$
      \begin{itemize}
      \item The measurement equation is the relationship between observations ($y_t$) and state (=structure) $l_t$
      \end{itemize}
    \item \textbf{State equation} $l_t = l_{t-1} + \alpha*\epsilon_t$, where $\epsilon_t \sim \mathcal{N}(0, \sigma^2)$
      \begin{itemize}
      \item The state equation is evolution of the state variable ($l_t$) through time
      \end{itemize}
     \end{itemize}
  \end{block}


  \begin{itemize}
  \item Both equations have the same error process $\epsilon$
  \end{itemize}
  
\end{frame}


\begin{frame}{State-Space Representation for Multiplicative Models}
  \begin{itemize}
  \item Instead of differential errors, specify relative errors: $\eta_t = \frac{y_t - \hat{y}_{t|t-1}}{ \hat{y}_{t|t-1}}$
  \item Some easy algebra, substituting $\hat{y}_{t|t-1} = l_{t-1}$ gives   
  \end{itemize}

  \begin{block}{State-Space Representation}
    \begin{itemize}
    \item \textbf{Measurement equation}: $y_t = l_{t-1}(1+\epsilon_t)$
    \item \textbf{State equation}: $l_t = l_{t-1}(1+\alpha*\epsilon_t)$
    \end{itemize}
  \end{block}

Implication: Models with additivative and multiplicative errors with the same parameters generate \textbf{the same point forecasts but with different prediction intervals}
  
\end{frame}

% Add a few charts


\section{Models with Trend}

\begin{frame}{Holt's Linear Trend}

  \begin{block}{Component Form}
    \begin{itemize}
    \item Level: $l_t = \alpha_t + (1-\alpha)(l_{t-1}+ b_{t-1})$
    \item Trend: $b_t = \beta*(l_t - l_{t-1}) + (1-\beta)b_{t-1}$
    \item Forecast: $\hat{y}_{t+h|t}=  l_t + hb_t$
    \end{itemize}
  \end{block}

  \begin{itemize}
  \item Two smoothing parameters: $\alpha$ and $\beta$ ($0 \leq \alpha, \ \beta \leq 1$)
  \item $l_t$ level: weighted average between $y_t$ and one-step ahead forecast for time $t$: $l_{t-1} + b_{t-1} = \hat{y}_{t|t-1}$
  \item $b_t$ slope: weighted average of $(l_t - l_{t-1})$ and $b_{t-1}$, current and previous estimate of slope
  \item Choose $\alpha, \ \beta, \ l_0, \ b_0$ to mininize the SSE (sum squared errors)
  \end{itemize}
\end{frame}

% Add a few charts



\begin{frame}{Damped Trend Method}

  \begin{block}{Component Form}
    \begin{itemize}
    \item $l_t = \alpha y_t + (1-\alpha)(l_{t-1} + \phi b_{t-1})$
    \item $b_t = \beta*(l_t - l_{t-1}) + (1-\beta)\phi b_{t-1}$
    \item $\hat{y}_{t+h|t} = l_t + (\phi + \phi^2 + \dots + \phi^h)b_t$
    \end{itemize}
  \end{block}

  \begin{itemize}
  \item $\phi$ is the damping parameter, $0 \leq \phi \leq 1$
  \item If $\phi=1$, the method boils down to Holt's linear trend
  \item As $h \to \infty$, then $\hat{y}_{T+h|T} \ \to \ l_T + \frac{\phi b_T}{1-\phi}$
  \item Application: short-run forecasts are trended, long-run forecasts constant
  \end{itemize}
  
  
\end{frame}


\begin{frame}{Holt-Winters Seasonal Model}

  Holt and Winters extended Holt's method to capture seasonality
  
  \begin{block}{Component Form}
    \begin{itemize}
    \item Level: $l_t = \alpha (y_t - s_{t-m}) + (1- \alpha )(l_{t-1} + b_{t-1})$
    \item Trend: $b_t = \beta(l_t - l_{t-1}) + (1-\beta)b_{t-1}$
    \item Season: $\gamma(y_t - l_{t-1} - b_{t-1}) + (1-\gamma)s_{t-m}$
    \item Forecast: $\hat{y}_{t+h|t} = l_t + hb_t + s_{t+h - m(k+1)}$
    \end{itemize}
  \end{block}

  \begin{itemize}
  \item $m$ is the period of seasonality (e.g. $m = 4$ for quarterly data)
  \end{itemize}
  
\end{frame}


\begin{frame}{Seasonal Component}
  \begin{itemize}
  \item The seasonal component is usually expressed as:
    \begin{equation*}
      s_t = \gamma*(y_t - l_t) + (1-\gamma)*s_{t-m} \qquad 0 \ \leq \ \gamma \ \leq 1
    \end{equation*}
  \item By subtitution, we can derive the dynamic of the seasonal term as:
    \begin{equation*}
      s_t = \gamma*(1-\alpha)(y_t - l_{t-1} - b_{t-1}) + [1 - \gamma(1-\alpha)] s_{t-m}
    \end{equation*}
  \end{itemize}  
\end{frame}




\begin{frame}{Forecasting with ETS models}
\textbf{Traditional point forecasts}: iterate the equations for $t = T+1, T+2, \dots, T+h$. By construction, $\epsilon_t = 0 \ \forall \ t>T$

\begin{itemize}
\item Equals to $E[y_{t+h}|x_t]$ in the case of additive seasonality only
\item Point forecasts for additive ETS are the same as for multiplicative ETS if the parameters are the same
\end{itemize}
  
\end{frame}



\begin{frame}{Prediction Intervals}

  \begin{itemize}
  \item They can only be generated using the models
  \item The prediction intervals will differ between models with additive and multiplicative errors
  \item Some simple ETS models offer exact formula
  \item For more complex ETS models, the only solution for generating the confidence intervals is by bootstrapping
    \begin{itemize}
    \item Simulate future sample paths, conditional on the last estimates of the states
    \item Obtain the prediction intervals from the percentiles of these simulated future paths
    \end{itemize}
  \end{itemize}
  
\end{frame}


\begin{frame}
  \frametitle{Main Idea: Control the Rate of Change}

  \begin{itemize}
  \item $\alpha$ controls the flexibility of the \textbf{level}
    \begin{itemize}
    \item If $\alpha = 0$, the level never updates (stays at the mean)
    \item If $\alpha = 1$, the level updates completely (naive, start from yesterday)
    \end{itemize}
  \item $\beta$ controls the flexibility of the \textbf{trend}
    \item If $\beta = 0$, the trend is linear
    \item If $\beta = 1$, the trend changes suddenly at each observation
    
    \item $\gamma$ controls the flexibility of the \textbf{seasonality}
      \begin{itemize}
      \item If $\gamma = 0$ the seasonality is fixed (seasonal mean)
      \item If $\gamma = 1$ the seasonality updates completely (seasonal naive)        
      \end{itemize}
  \end{itemize}
  
\end{frame}


\section{ARIMA Model}


\begin{frame}{ARIMA Model}

  \begin{itemize}
  \item AR: autoregressive (lagged observations as inputs)
  \item I: integrated (differencing to make series stationary)
  \item MA: moving average (lagged errors as inputs)
  \end{itemize}

  \begin{block}{Intuition}
    Contrary to an ETS, an ARIMA model is rarely interpretable in terms of visible data structures like trend and seasonality. But it can capture a huge range of time series patterns
  \end{block}  
\end{frame}


\begin{frame}{Refresher: Intuitive Definition of Stationarity}

  \begin{block}{Intuitive Characterization}
    If ${y_t}$ is a stationary time series, then for any period $s$ in the future, the distribution $\{y_t, \dots, y_{t+s}\}$ doesn't depend on $t$
  \end{block}

  A \textbf{stationary series} is:
  \begin{itemize}
  \item Roughly horizontal
  \item Constant variance
  \item No predictable patterns in the long term
  \end{itemize}

  Stabilization: 
  \begin{itemize}
  \item Transformations help to \textbf{stabilize the variance}
  \item For ARIMA modelling, we also need to \textbf{stabilize the mean}
  \end{itemize}
  
\end{frame}


\begin{frame}{Identifying Non-Stationary Series}

  Tips:\\
  
  \begin{itemize}
  \item Time plot
  \item The ACF of stationary data drops to zero relatively quickly
  \item The ACF of non-stationary data decreases slowly
  \item For non-stationary data, the value of the first coefficient is often large and positive
  \end{itemize}

  
\end{frame}


% Put a bunch of charts


\begin{frame}{Differencing}
  \begin{wideitemize}
  \item Differencing helps to \textbf{stabilize the mean}
  \item The differenced series is the \emph{change} (or first difference) between each observation in the original series: $y'_t = y_t - y_{t-1}$
  \item The differenced series will have only $T-1$ values since it is not possible to calculate a difference $y'_1$ for the first observation
  \end{wideitemize}

\end{frame}


\begin{frame}{Random Walk Model}
  If the differenced series is white noise with zero mean:
  \begin{block}{Specification}
    \begin{equation*}
      y_t - y_{t-1} = \epsilon_t \qquad \text{where} \ \epsilon_t \ \sim \ \mathcal{N}(0, \sigma^2)
    \end{equation*}
  \end{block}

  \begin{itemize}
  \item Very widely used for non-stationary data
  \item This is the model behind the \textbf{naive method}
  \item Random walks typically have:
    \begin{itemize}
    \item Long periods of apparent trends up or down
    \item Sudden/unpredictable chanes in direction
    \end{itemize}
  \item In a random walk, the forecast are equal to the last observation
    \begin{itemize}
    \item Future movements up or down are equally likely
    \end{itemize}
  \end{itemize}   
\end{frame}


\begin{frame}{Random Walk with Drift Model}
  If the differenced series has a drift $c$:

  \begin{equation*}
    y_t - y_{t-1} = c + \epsilon_t \qquad \text{where} \ \epsilon_t \ \sim \ \mathcal{N}(0, \sigma^2)
  \end{equation*}
  
  \begin{wideitemize}
  \item $c$ is the \textbf{average change} between consecutive observations
  \item If $c > 0$, $y_t$ will tend to drift upwards and vice-versa
  \item This is the model behind the \textbf{drift method} 
  \end{wideitemize}  
\end{frame}



\begin{frame}{Second-Order Differencing}

  Occasionally, the differenced data will not appear stationary and it may be necessary to difference the data a second time:\\

  \medskip
  
  \begin{equation*}
    y''_{t} = y'_t - y'_{t-1} = (y_t - y_{t-1}) - (y_{t-1} - y_{t-2})
  \end{equation*}

  \medskip

  \begin{itemize}
  \item $y''_{t}$ will have $T-2$ values
  \item In practice, it is almost never necessary to go beyond second-order differences
  \end{itemize}
  
\end{frame}


\begin{frame}{Seasonal Differencing}

  \begin{block}{Definition: Seasonal Difference}
    A seasonal difference is the difference between an observation and the corresponding observation from the previous year

    \begin{equation*}
      y'_t = y_t - y_{t-m}
    \end{equation*}

    where $m$ = number of seasons


    \begin{itemize}
    \item For monthly data, $m = 12$
    \item For quarterly data, $m = 4$      
    \end{itemize}
    
  \end{block}
\end{frame}


\begin{frame}{Differencing in Practice}

  When both seasonal and first differences are applied:

  \begin{wideitemize}
    \item It makes no difference which one is done first - the result will be the same
    \item If seasonality is strong, we recommend that seasonal differencing be done first because sometimes the resulting series will be stationary and there will be no need for further first difference
    \item It is important that, if differencing is used, the differences are \textbf{interpretable}: for instance, taking lag 3 differences for yearly data is difficult to interpret
  \end{wideitemize}
\end{frame}


\begin{frame}{Unit Root Tests}
  Statistical tests can be used to determine the required order of differencing

  \begin{wideenumerate}
    \item \textbf{Augmented Dickey Fuller test}: null hypothesis is that the data is non-stationary and non-seasonal
    \item KPSS (Kwiatkowski-Phillips-Schmidt Shin) test: the null hypothesis is that the data is stationary is non-seasonal
    \item Other tests are available for seasonal data
  \end{wideenumerate}
  
  
\end{frame}



\begin{frame}{Backshift Notation}

  \begin{block}{Notation}
    The backshift notational device, $B$ is used as follows:\\

    \begin{equation*}
      B y_t = y_{t-1}
    \end{equation*}
  \end{block}

    
    \begin{itemize}
    \item   $B$ operating on $y_t$ has the effect of \textbf{shifting the data back one period}
    \item   Two applications of $B$ to $y_t$ shifts the data back \textbf{two periods}
      \begin{equation*}
        B(By_t) = B^2 y_t = y_{t-2}
      \end{equation*}
    \end{itemize}
    
$B$ depends on the period/frequency considered. Shifting monthly data by a year supposes using $B^{12}$

\end{frame}


\begin{frame}{Relationship with Differencing}
  Importantly, the backshift operator is convenient for describing differencing\\

  \medskip
  
  \begin{exampleblock}{Backshift Operator and Differencing}
    $$y'_t = y_t - y_{t-1} = y_t - By_t = (1-B)y_t$$
  \end{exampleblock}


  \begin{itemize}
    \item Likewise, second-order differences are obtained with: $y''_t = (1-B)^2y_t$  
    \item Pay attention !! Second-order difference is not second difference 
      \begin{itemize}
      \item Second order difference: $(1-B)^2 y_t = y''_t = (y_t - y_{t-1}) - (y_{t-1} - y_{t-2})$
      \item Second difference: $1-B^2 y_t = y_t - y_{t-2}$
      \end{itemize}
  \end{itemize}
  
\end{frame}


\begin{frame}{Combined Effects}

Assume that you want to combine a first difference with a seasonal difference:
  
  \begin{itemize}
  \item First difference: $(1-B)$
  \item Seasonal difference: $(1-B^m)$
  \end{itemize}

  Then, the backshift operator can be directly combined, as a polynomial, to represent the resulting time series:

  \begin{equation*}
    (1-B)(1-B^m)y_t = (1 - B - B^m + B^{m+1})y_t = y_t - y_{t-1} - y_{t-m} + y_{t-m-1}
  \end{equation*}    
\end{frame}


\section{Autoregressive Models}






\end{document}



%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
